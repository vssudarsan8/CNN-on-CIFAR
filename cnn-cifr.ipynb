{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSvKMyC1b2YN"
   },
   "source": [
    "# Assignment 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "Uee4vFbzmYu_",
    "outputId": "7f9da4b6-4711-4e61-a8bd-4552ca06e03b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UqGmMJfkjxlN"
   },
   "outputs": [],
   "source": [
    "# import keras\n",
    "# from keras.datasets import cifar10\n",
    "# from keras.models import Model, Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "# from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "# from keras.layers import Concatenate\n",
    "# from keras.optimizers import Adam\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nBT59LPCj4FY"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "l = 40\n",
    "num_filter = 12\n",
    "compression = 0.5\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztLKFZdhb2Yi"
   },
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "__VldHAskRNo",
    "outputId": "e2e0a460-2b12-4719-a2e4-14aaa2dce56e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 13s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR10 Data\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
    "\n",
    "# convert to one hot encoing \n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "USoLa4_Lj4H0",
    "outputId": "29128ea9-a927-4b12-a226-4d87587fca86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jQ6Or9Glb2Yv"
   },
   "source": [
    "#### Standarding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v7u77XEXn3JG"
   },
   "outputs": [],
   "source": [
    "def prep_pixels(train, test):\n",
    "# convert from integers to floats\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "# normalize to range 0-1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "# return normalized images\n",
    "    return train_norm, test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pjip8_PuoTIG"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train,X_test=prep_pixels(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "id": "Vzn95rmMj4KE",
    "outputId": "98807a1b-6dff-428e-ad02-c008aa3a7172"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU4AAAD7CAYAAAAFI30bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/\n",
    "from numpy import expand_dims\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "c=X_train[1]\n",
    "c.shape\n",
    "samples = expand_dims(c, 0)\n",
    "# create image data augmentation generator\n",
    "datagen = ImageDataGenerator(rotation_range=90)\n",
    "# prepare iterator\n",
    "it = datagen.flow(samples, batch_size=1)\n",
    "# generate samples and plot\n",
    "for i in range(9):\n",
    "    pyplot.subplot(330 + 1 + i)\n",
    "# generate batch of images\n",
    "    batch = it.next()\n",
    "# convert to unsigned integers for viewing\n",
    "    image = batch[0].astype('uint8')\n",
    "# plot raw pixel data\n",
    "    pyplot.imshow(image)\n",
    "# show the figure\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KVYFMN2_ZXOv"
   },
   "outputs": [],
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P-BqhGBZb2ZG"
   },
   "source": [
    " ### Model using dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ATDUK5vj4Mh"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def denseblock(input, num_filter = 64, dropout_rate = 0):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l): \n",
    "        BatchNorm = layers.BatchNormalization()(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (5,5),kernel_initializer=\"he_uniform\" ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp\n",
    "\n",
    "## transition Blosck\n",
    "def transition(input, num_filter = 32, dropout_rate = 0):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (5,5), kernel_initializer=\"he_uniform\" ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "#output layer\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    flat = layers.Flatten()(AvgPooling)\n",
    "    output = layers.Dense(num_classes, activation='softmax')(flat)\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0fZacgTQk_Gl"
   },
   "outputs": [],
   "source": [
    "num_filter = 10\n",
    "dropout_rate = 0\n",
    "l = 12\n",
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.Conv2D(num_filter, (5,5), use_bias=False ,padding='same')(input)\n",
    "BatchNorm = layers.BatchNormalization()(First_Conv2D)\n",
    "\n",
    "First_Block = denseblock(BatchNorm,32, dropout_rate)\n",
    "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, 16, dropout_rate)\n",
    "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
    "conv_layer = layers.Conv2D(num_filter, (1,1), use_bias=False ,padding='same') (input)\n",
    "last = layers.GlobalMaxPooling2D()(conv_layer)\n",
    "output = layers.Activation('softmax')(last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "voLWzMSFj4Oy",
    "outputId": "0e714f8a-70fe-4fed-dfdd-5def78252ed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_215 (Conv2D)          (None, 32, 32, 10)        30        \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_1 (Glob (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "activation_208 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 30\n",
      "Trainable params: 30\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c0QWFrXej4RG"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(), metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CTy-dtN4j4Tc",
    "outputId": "47534dc4-5dd5-4769-db43-095fab35e86d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "780/781 [============================>.] - ETA: 0s - loss: 2.3086 - acc: 0.1021Epoch 1/15\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 2s 153us/sample - loss: 2.2888 - acc: 0.1032\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 2.3086 - acc: 0.1021 - val_loss: 2.2968 - val_acc: 0.1032\n",
      "Epoch 2/15\n",
      "780/781 [============================>.] - ETA: 0s - loss: 2.2958 - acc: 0.1090Epoch 1/15\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 68us/sample - loss: 2.2898 - acc: 0.1346\n",
      "781/781 [==============================] - 19s 24ms/step - loss: 2.2958 - acc: 0.1091 - val_loss: 2.2929 - val_acc: 0.1346\n",
      "Epoch 3/15\n",
      "780/781 [============================>.] - ETA: 0s - loss: 2.2923 - acc: 0.1288Epoch 1/15\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 66us/sample - loss: 2.2835 - acc: 0.1378\n",
      "781/781 [==============================] - 19s 24ms/step - loss: 2.2923 - acc: 0.1288 - val_loss: 2.2896 - val_acc: 0.1378\n",
      "Epoch 4/15\n",
      "780/781 [============================>.] - ETA: 0s - loss: 2.2891 - acc: 0.1359Epoch 1/15\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 69us/sample - loss: 2.2830 - acc: 0.1553\n",
      "781/781 [==============================] - 19s 24ms/step - loss: 2.2891 - acc: 0.1359 - val_loss: 2.2862 - val_acc: 0.1553\n",
      "Epoch 5/15\n",
      "778/781 [============================>.] - ETA: 0s - loss: 2.2857 - acc: 0.1446Epoch 1/15\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 68us/sample - loss: 2.2813 - acc: 0.1320\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 2.2857 - acc: 0.1445 - val_loss: 2.2830 - val_acc: 0.1320\n",
      "Epoch 6/15\n",
      "779/781 [============================>.] - ETA: 0s - loss: 2.2825 - acc: 0.1471Epoch 1/15\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 70us/sample - loss: 2.2778 - acc: 0.1554\n",
      "781/781 [==============================] - 19s 24ms/step - loss: 2.2825 - acc: 0.1472 - val_loss: 2.2792 - val_acc: 0.1554\n",
      "Epoch 7/15\n",
      "779/781 [============================>.] - ETA: 0s - loss: 2.2789 - acc: 0.1503Epoch 1/15\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 70us/sample - loss: 2.2778 - acc: 0.1560\n",
      "781/781 [==============================] - 19s 24ms/step - loss: 2.2789 - acc: 0.1502 - val_loss: 2.2754 - val_acc: 0.1560\n",
      "Epoch 8/15\n",
      "780/781 [============================>.] - ETA: 0s - loss: 2.2750 - acc: 0.1525Epoch 1/15\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 75us/sample - loss: 2.2740 - acc: 0.1551\n",
      "781/781 [==============================] - 19s 24ms/step - loss: 2.2750 - acc: 0.1526 - val_loss: 2.2713 - val_acc: 0.1551\n",
      "Epoch 9/15\n",
      "779/781 [============================>.] - ETA: 0s - loss: 2.2704 - acc: 0.1503Epoch 1/15\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 73us/sample - loss: 2.2628 - acc: 0.1549\n",
      "781/781 [==============================] - 19s 24ms/step - loss: 2.2704 - acc: 0.1502 - val_loss: 2.2667 - val_acc: 0.1549\n",
      "Epoch 10/15\n",
      "780/781 [============================>.] - ETA: 0s - loss: 2.2663 - acc: 0.1525Epoch 1/15\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 69us/sample - loss: 2.2589 - acc: 0.1568\n",
      "781/781 [==============================] - 19s 24ms/step - loss: 2.2663 - acc: 0.1525 - val_loss: 2.2626 - val_acc: 0.1568\n",
      "Epoch 11/15\n",
      "778/781 [============================>.] - ETA: 0s - loss: 2.2622 - acc: 0.1546Epoch 1/15\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 69us/sample - loss: 2.2533 - acc: 0.1534\n",
      "781/781 [==============================] - 19s 24ms/step - loss: 2.2622 - acc: 0.1546 - val_loss: 2.2588 - val_acc: 0.1534\n",
      "Epoch 12/15\n",
      "780/781 [============================>.] - ETA: 0s - loss: 2.2585 - acc: 0.1528Epoch 1/15\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 74us/sample - loss: 2.2526 - acc: 0.1563\n",
      "781/781 [==============================] - 19s 24ms/step - loss: 2.2585 - acc: 0.1527 - val_loss: 2.2552 - val_acc: 0.1563\n",
      "Epoch 13/15\n",
      "778/781 [============================>.] - ETA: 0s - loss: 2.2550 - acc: 0.1565Epoch 1/15\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 74us/sample - loss: 2.2438 - acc: 0.1568\n",
      "781/781 [==============================] - 19s 24ms/step - loss: 2.2550 - acc: 0.1565 - val_loss: 2.2517 - val_acc: 0.1568\n",
      "Epoch 14/15\n",
      "778/781 [============================>.] - ETA: 0s - loss: 2.2514 - acc: 0.1572Epoch 1/15\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 78us/sample - loss: 2.2364 - acc: 0.1552\n",
      "781/781 [==============================] - 19s 24ms/step - loss: 2.2514 - acc: 0.1573 - val_loss: 2.2482 - val_acc: 0.1552\n",
      "Epoch 15/15\n",
      "779/781 [============================>.] - ETA: 0s - loss: 2.2483 - acc: 0.1597Epoch 1/15\n",
      "10000/781 [================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 76us/sample - loss: 2.2366 - acc: 0.1569\n",
      "781/781 [==============================] - 19s 24ms/step - loss: 2.2483 - acc: 0.1597 - val_loss: 2.2452 - val_acc: 0.1569\n",
      "> 15.690\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMMAAAEICAYAAADiJ0BpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/\n",
    "def summarize_diagnostics(history):\n",
    "\t# plot loss\n",
    "\tpyplot.subplot(121)\n",
    "\tpyplot.title('Cross Entropy Loss')\n",
    "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "\tpyplot.show()\n",
    "\t\n",
    " \n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t\n",
    "\t# define model\n",
    "\t\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "\t# prepare iterator\n",
    "\tit_train = datagen.flow(X_train, y_train, batch_size=64)\n",
    "\t# fit model\n",
    "\tsteps = int(X_train.shape[0] / 64)\n",
    "\thistory = model.fit_generator(it_train, steps_per_epoch=steps, epochs=15, validation_data=(X_test, y_test), verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# learning curves\n",
    "\tsummarize_diagnostics(history)\n",
    " \n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4F8lCb6ab2Zc"
   },
   "source": [
    "train_loss: 0.1203   train_acc: 0.9561 \n",
    "\n",
    "val_loss: 0.6271   val_acc: 0.8439"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wkJv3UK3b2Zd"
   },
   "source": [
    "### Model without dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "59osdY7zj4Vr"
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "# Dense Block\n",
    "def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l): \n",
    "        BatchNorm = layers.BatchNormalization()(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (5,5), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp\n",
    "\n",
    "## transition Blosck\n",
    "def transition(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (5,5), use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "#output layer\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    AvgPooling = layers. MaxPooling2D(pool_size=(2,2))(relu)\n",
    "    \n",
    "    output = layers.Conv2D(filters=10,kernel_size=(2,2),activation='softmax')(AvgPooling)\n",
    "   \n",
    "    flat = layers.Flatten()(output)    \n",
    "    return flat\n",
    "\n",
    "\n",
    "\n",
    "num_filter = 12\n",
    "dropout_rate = 0\n",
    "l = 12\n",
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.Conv2D(32, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "First_Block = denseblock(First_Conv2D,10, dropout_rate)\n",
    "First_Transition = transition(First_Block, 64, dropout_rate)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, 10, dropout_rate)\n",
    "Second_Transition = transition(Second_Block, 32, dropout_rate)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "Third_Transition = transition(Third_Block, 32, dropout_rate)\n",
    "\n",
    "conv_layer = layers.Conv2D(num_filter, (1,1), use_bias=False ,padding='same')(input)\n",
    "last = layers.GlobalMaxPooling2D()(conv_layer)\n",
    "output = layers.Activation('softmax')(last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "P9Ww0AmJj4Zj",
    "outputId": "ec19b7c0-9d0e-49ee-c900-83ac6ed5fb84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_256 (Conv2D)          (None, 32, 32, 12)        36        \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_2 (Glob (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "activation_248 (Activation)  (None, 12)                0         \n",
      "=================================================================\n",
      "Total params: 36\n",
      "Trainable params: 36\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1WQT_osgghe5"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "#a=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YIZBdVo-gwLV"
   },
   "outputs": [],
   "source": [
    "def summarize_diagnostics(history):\n",
    "\t# plot loss\n",
    "\tpyplot.subplot(121)\n",
    "\tpyplot.title('Cross Entropy Loss')\n",
    "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "\tpyplot.show()\n",
    "\t\n",
    " \n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t\n",
    "\t# define model\n",
    "\t\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "\t# prepare iterator\n",
    "\tit_train = datagen.flow(X_train, y_train, batch_size=60)\n",
    "\t# fit model\n",
    "\tsteps = int(X_train.shape[0] / 39)\n",
    "\thistory = model.fit_generator(it_train, steps_per_epoch=steps, epochs=20, validation_data=(X_test, y_test), verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# learning curves\n",
    "\tsummarize_diagnostics(history)\n",
    " \n",
    "# entry point, run the test harness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J5XdYTZ_b2Zv"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "#saving model weights \n",
    "model.save('my_model.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t4uFEDxOb2Z6"
   },
   "source": [
    " ###### JUST CONTINUING THE MODEL FOR ANOTHER 5 EPOCHS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P4ZIxDdaMXEK"
   },
   "outputs": [],
   "source": [
    "\n",
    "def summarize_diagnostics(history):\n",
    "\t# plot loss\n",
    "\tpyplot.subplot(121)\n",
    "\tpyplot.title('Cross Entropy Loss')\n",
    "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "\tpyplot.show()\n",
    "\t\n",
    " \n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t\n",
    "\t# define model\n",
    "\t\n",
    "\t# create data generator\n",
    "\tdatagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "\t# prepare iterator\n",
    "\tit_train = datagen.flow(X_train, y_train, batch_size=60)\n",
    "\t# fit model\n",
    "\tsteps = int(X_train.shape[0] / 39)\n",
    "\thistory = model.fit_generator(it_train, steps_per_epoch=steps, epochs=5, validation_data=(X_test, y_test), verbose=1)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# learning curves\n",
    "\tsummarize_diagnostics(history)\n",
    " \n",
    "# entry point, run the test harness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YnQISTa0b2aC"
   },
   "source": [
    "train_loss: 0.0350               train_acc: 0.9877 \n",
    "        \n",
    "val_loss: 0.5632       val_acc: 0.8954"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "5NOtb1Nob2aE",
    "outputId": "0a60450c-fa85-45d1-f594-dd51cbc3a453"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+--------+------------+-----------+-----------+----------+\n",
      "|           Model           | epochs | train_loss | train acc | test loss | test acc |\n",
      "+---------------------------+--------+------------+-----------+-----------+----------+\n",
      "|   model with dense layer  |   75   |   0.1203   |   0.956   |   0.6271  |  0.843   |\n",
      "| model without dense layer |  205   |   0.035    |   0.987   |   0.563   |  0.895   |\n",
      "+---------------------------+--------+------------+-----------+-----------+----------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "conclusion= PrettyTable()\n",
    "conclusion.field_names = [ \"Model\", 'epochs','train_loss','train acc',\"test loss\",'test acc']\n",
    "\n",
    "conclusion.add_row([\"model with dense layer\", 75,0.1203, 0.956, 0.6271,0.843])\n",
    "conclusion.add_row([\"model without dense layer\",205, 0.035, 0.987,0.563,0.895])\n",
    "\n",
    "\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zaeQwNukb2aK"
   },
   "source": [
    "# Conclusion \n",
    "\n",
    "> Overfitting is one the problem in this assignment since dropouts was excluded.\n",
    "\n",
    "> Even tried using L2 regularization but still models are overfitting.\n",
    "\n",
    "> Loss on test data doesnot change after certain number of iterations which can be seen from plots.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn_cifr.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
